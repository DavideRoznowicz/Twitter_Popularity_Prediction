sum(trainino$favorite_count>1000)
log_favorite=log(trainino$favorite_count)
hist(log_favorite)
hist(log_favorite,breaks = 100)
hist(log_favorite,breaks = 50)
hist(log_retweet,breaks = 50)
log_retweet=log(trainino$retweet_count+1)
hist(log_retweet,breaks = 50)
trainino=data.frame(#status_id,
retweet_count,
#favorite_count,
#text,
display_text_width,
mention,hashtags,
followers,
friends,
lists,
link)
trainino$spettro=data.matrix(spettro[,1:50])
###########################################################################################################
text=dataset$text
text=gsub("http[^>]+$"," ",text) #remove last link
text=gsub("http[^>]+ "," ",text) #remove other link
text=gsub("@[^>]+$"," ",text) #remove last @
text=gsub("@[^>]+ "," ",text) #remove other @
text=gsub("#"," ",text)
text=tolower(text)
text=str_replace_all(text, "[\r\n]" , " ")
text=stemDocument(text, language = "english")
text=removeWords(text,stopwords(source = "smart"))
text=removeWords(text,stopwords("en"))
text=stripWhitespace(text)
text=gsub("-"," ",text)
text=gsub("'"," ",text)
#text=gsub("."," ",text)
text=gsub("…"," ",text)
text=gsub("[|]"," ",text)
text=gsub("pm"," ",text)
text=gsub("am"," ",text)
text=removeNumbers(text)
text=removePunctuation(text,ucp = TRUE)
ch=c("a")
for(i in 98:122)
{
ch=c(ch,intToUtf8(i))
}
text=stemDocument(text, language = "english")
text=removeWords(text,ch)
#text=corpus(text)
token_text <- tokens(text)
dfm_text <- dfm(token_text)
nomi_colonne=intersect(featnames(dfm_text),dict)
df=convert(dfm_text,to="data.frame")
to_plot=subset(df, select = c(nomi_colonne)) #prendiamo solo l'intersezione con il dictionary
to_plot=colSums(to_plot) #sommiamo le righe
occ_cucina=data.frame(to_plot)
sum(trainino$retweet_count>=0)
sum(trainino$retweet_count>=1)
sum(trainino$retweet_count>=10)
sum(trainino$retweet_count>=10)/10125
sum(trainino$retweet_count>=50)/10125
sum(trainino$retweet_count>=100)/10125
sum(trainino$retweet_count>=1000)/10125
traino$retweet_count[trainino$retweet_count==0]
trainino$retweet_count[trainino$retweet_count==0]
trainino$retweet_count[trainino$retweet_count==0]=0
trainino$retweet_count[trainino$retweet_count>0 & trainino$retweet_count<=10]=1
trainino$retweet_count[trainino$retweet_count>10 & trainino$retweet_count=<100]=2
trainino$retweet_count[trainino$retweet_count>10 & trainino$retweet_count<=100]=2
trainino$retweet_count[trainino$retweet_count>100]=3
View(trainino)
###########################################################################################################
text=dataset$text
text=gsub("http[^>]+$"," ",text) #remove last link
text=gsub("http[^>]+ "," ",text) #remove other link
text=gsub("@[^>]+$"," ",text) #remove last @
text=gsub("@[^>]+ "," ",text) #remove other @
text=gsub("#"," ",text)
text=tolower(text)
text=str_replace_all(text, "[\r\n]" , " ")
text=stemDocument(text, language = "english")
text=removeWords(text,stopwords(source = "smart"))
text=removeWords(text,stopwords("en"))
text=stripWhitespace(text)
text=gsub("-"," ",text)
text=gsub("'"," ",text)
#text=gsub("."," ",text)
text=gsub("…"," ",text)
text=gsub("[|]"," ",text)
text=gsub("pm"," ",text)
text=gsub("am"," ",text)
text=removeNumbers(text)
text=removePunctuation(text,ucp = TRUE)
ch=c("a")
for(i in 98:122)
{
ch=c(ch,intToUtf8(i))
}
text=stemDocument(text, language = "english")
text=removeWords(text,ch)
#text=corpus(text)
token_text <- tokens(text)
dfm_text <- dfm(token_text)
nomi_colonne=intersect(featnames(dfm_text),dict)
df=convert(dfm_text,to="data.frame")
to_plot=subset(df, select = c(nomi_colonne)) #prendiamo solo l'intersezione con il dictionary
to_plot=colSums(to_plot) #sommiamo le righe
occ_cucina=data.frame(to_plot)
#to_plot=to_plot/sum(to_plot)
library("wordcloud")
wordcloud(words =rownames(occ_cucina), freq =occ_cucina[,1],min.freq = 1,
max.words=400, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
nomi_colonne=intersect(featnames(dfm_text),outersect(featnames(dfm_text),dict))
to_plot=subset(df, select = c(nomi_colonne))
to_plot=colSums(to_plot)
occ=data.frame(to_plot)
#to_plot=to_plot/sum(to_plot)
wordcloud(words =rownames(occ), freq =occ[,1],
max.words=400, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
parole_cibo=rownames(occ_cucina)[occ_cucina$to_plot>5]
parole_altro=rownames(occ)[occ$to_plot>150]
filtro=c(parole_altro,parole_cibo)
set_parole=df[filtro]
x_train <- cbind(set_parole, trainino)
x_train=data.table(x_train)
indici=sample(seq_len(nrow(trainino)), size = 7000)
train=x_train[indici]
xgb <- RFTrainer$new(n_estimators = 200,classification=1)
xgb$fit(train, "retweet_count")
predictions <- xgb$predict(test)
predictions[1]
xgb <- RFTrainer$new(n_estimators = 300,classification=1)
xgb$fit(train, "retweet_count")
predictions <- xgb$predict(test)
predicttions[1:10]
predictions[1:10]
trainino$retweet_count[1:10]
predictions[10:80]
trainino$retweet_count[10:80]
plot(predictions[1:100])
plot(as.numeric(predictions[1:100]))
plot(as.numeric(predictions[1:50]))
plot(as.numeric(predictions[1:50]))
points(as.numeric(trainino$retweet_count[1:50]))
plot(as.numeric(predictions[1:50]))
points(as.numeric(trainino$retweet_count[1:50]),col="red")
predictions[1]
predictions[1:10]
predictions[1:100]
plot(as.numeric(predictions[1:50]))
points(as.numeric(trainino$retweet_count[1:50]),col="red")
(as.numeric(predictions[1:50]))
plot(as.numeric(predictions[1:50]))
points(as.numeric(trainino$retweet_count[1:50]),col="red")
plot(as.numeric(predictions[1:50]))
(as.numeric(predictions[1:50])
)
(predictions[1:50])
as.numeric(trainino$retweet_count[1:50])
trainino$retweet_count[1:50]
plot(as.numeric(predictions[1:50])-1)
points(as.numeric(trainino$retweet_count[1:50]),col="red")
points(as.numeric(trainino$retweet_count[1:50]),col="red",pch = 4)
plot(as.numeric(predictions[1:50])-1)
points(as.numeric(trainino$retweet_count[1:50]),col="red",pch = 4)
sum(trainino$retweet_count==0)
sum(trainino$retweet_count==1)
sum(trainino$retweet_count==2)
sum(trainino$retweet_count==3)
plot(as.numeric(predictions[1:90])-1)
points(as.numeric(trainino$retweet_count[1:90]),col="red",pch = 4)
sum(trainino$retweet_count>=5)
trainino=data.frame(#status_id,
retweet_count,
#favorite_count,
#text,
display_text_width,
mention,hashtags,
followers,
friends,
lists,
link)
trainino$retweet_count>=5
sum(trainino$retweet_count>=5)
sum(trainino$retweet_count>=5)/10125
sum(trainino$retweet_count>=10)/10125
sum(trainino$retweet_count>=5)/10125
sum(trainino$retweet_count>5)/10125
trainino$retweet_count[trainino$retweet_count>5]=0
trainino$retweet_count[trainino$retweet_count<=5]=1
trainino$retweet_count[trainino$retweet_count>5]=0
trainino$retweet_count[trainino$retweet_count<=5]=1
xgb <- RFTrainer$new(n_estimators = 300,classification=1)
xgb$fit(train, "retweet_count")
predictions <- xgb$predict(test)
plot(as.numeric(predictions[1:90])-1)
points(as.numeric(trainino$retweet_count[1:90]),col="red",pch = 4)
predictions[1:10]
trainino$retweet_count[1:10]
sum(predistion!=trainino$retweet_count)
sum(prediction!=trainino$retweet_count)
sum(predictions!=trainino$retweet_count)
sum(predictions!=trainino$retweet_count)
sum(predictions!=train$retweet_count)
train$retweet_count
trainino=data.frame(#status_id,
retweet_count,
#favorite_count,
#text,
display_text_width,
mention,hashtags,
followers,
friends,
lists,
link)
trainino$spettro=data.matrix(spettro[,1:50])
trainino$retweet_count[trainino$retweet_count>5]=0
trainino$retweet_count[trainino$retweet_count<=5]=1
###########################################################################################################
text=dataset$text
text=gsub("http[^>]+$"," ",text) #remove last link
text=gsub("http[^>]+ "," ",text) #remove other link
text=gsub("@[^>]+$"," ",text) #remove last @
text=gsub("@[^>]+ "," ",text) #remove other @
text=gsub("#"," ",text)
text=tolower(text)
text=str_replace_all(text, "[\r\n]" , " ")
text=stemDocument(text, language = "english")
text=removeWords(text,stopwords(source = "smart"))
text=removeWords(text,stopwords("en"))
text=stripWhitespace(text)
text=gsub("-"," ",text)
text=gsub("'"," ",text)
#text=gsub("."," ",text)
text=gsub("…"," ",text)
text=gsub("[|]"," ",text)
text=gsub("pm"," ",text)
text=gsub("am"," ",text)
text=removeNumbers(text)
text=removePunctuation(text,ucp = TRUE)
ch=c("a")
for(i in 98:122)
{
ch=c(ch,intToUtf8(i))
}
text=stemDocument(text, language = "english")
text=removeWords(text,ch)
#text=corpus(text)
token_text <- tokens(text)
dfm_text <- dfm(token_text)
nomi_colonne=intersect(featnames(dfm_text),dict)
df=convert(dfm_text,to="data.frame")
to_plot=subset(df, select = c(nomi_colonne)) #prendiamo solo l'intersezione con il dictionary
to_plot=colSums(to_plot) #sommiamo le righe
occ_cucina=data.frame(to_plot)
#to_plot=to_plot/sum(to_plot)
library("wordcloud")
x_train <- cbind(set_parole, trainino)
x_train=data.table(x_train)
indici=sample(seq_len(nrow(trainino)), size = 7000)
train=x_train[indici]
test=x_train[-indici]
xgb <- RFTrainer$new(n_estimators = 300,classification=1)
xgb$fit(train, "retweet_count")
predictions <- xgb$predict(test)
sum(predictions!=test$retweet_count)/10125
sum(predictions!=test$retweet_count)
predictions[1:10]
predictions[1:100]
trainino$retweet_count[trainino$retweet_count>5]=0
trainino$retweet_count[trainino$retweet_count<=5]=1
trainino$retweet_count[trainino$retweet_count>5]=0
trainino$retweet_count[trainino$retweet_count<=5]=1
trainino=data.frame(#status_id,
retweet_count,
#favorite_count,
#text,
display_text_width,
mention,hashtags,
followers,
friends,
lists,
link)
trainino$spettro=data.matrix(spettro[,1:50])
trainino$retweet_count[trainino$retweet_count>5]=0
trainino$retweet_count[trainino$retweet_count<=5]=1
trainino$retweet_count[1:10]
trainino$retweet_count[1:100]
trainino$retweet_count[1:1000]
trainino$retweet_count[1:10000]
summary(strainino$retweet_count)
summary(trainino$retweet_count)
hashtags=!is.na(dataset$hashtags)#
mention=!is.na(dataset$mentions_screen_name)#
status_id=dataset$status_id#
retweet_count=dataset$retweet_count#
favorite_count=dataset$favorite_count #
text=dataset$text#
display_text_width=dataset$display_text_width#
link=!is.na(dataset$urls_t.co)#
spettro=dataset$image
followers=dataset$followers_count#
friends=dataset$friends_count#
lists=dataset$listed_count#
trainino=data.frame(#status_id,
retweet_count,
#favorite_count,
#text,
display_text_width,
mention,hashtags,
followers,
friends,
lists,
link)
trainino$retweet_count[1:10]
trainino$spettro=data.matrix(spettro[,1:50])
trainino$retweet_count[trainino$retweet_count>5]=0
trainino=data.frame(#status_id,
retweet_count,
#favorite_count,
#text,
display_text_width,
mention,hashtags,
followers,
friends,
lists,
link)
trainino$spettro=data.matrix(spettro[,1:50])
trainino$retweet_count[trainino$retweet_count<=5]=1
trainino$retweet_count[trainino$retweet_count>5]=0
summary(trainino$retweet_count)
###########################################################################################################
text=dataset$text
text=gsub("http[^>]+$"," ",text) #remove last link
text=gsub("http[^>]+ "," ",text) #remove other link
text=gsub("@[^>]+$"," ",text) #remove last @
text=gsub("@[^>]+ "," ",text) #remove other @
text=gsub("#"," ",text)
text=tolower(text)
text=str_replace_all(text, "[\r\n]" , " ")
text=stemDocument(text, language = "english")
text=removeWords(text,stopwords(source = "smart"))
text=removeWords(text,stopwords("en"))
text=stripWhitespace(text)
text=gsub("-"," ",text)
text=gsub("'"," ",text)
#text=gsub("."," ",text)
text=gsub("…"," ",text)
text=gsub("[|]"," ",text)
text=gsub("pm"," ",text)
text=gsub("am"," ",text)
text=removeNumbers(text)
text=removePunctuation(text,ucp = TRUE)
ch=c("a")
for(i in 98:122)
{
ch=c(ch,intToUtf8(i))
}
text=stemDocument(text, language = "english")
text=removeWords(text,ch)
#text=corpus(text)
token_text <- tokens(text)
dfm_text <- dfm(token_text)
nomi_colonne=intersect(featnames(dfm_text),dict)
df=convert(dfm_text,to="data.frame")
to_plot=subset(df, select = c(nomi_colonne)) #prendiamo solo l'intersezione con il dictionary
to_plot=colSums(to_plot) #sommiamo le righe
occ_cucina=data.frame(to_plot)
nomi_colonne=intersect(featnames(dfm_text),outersect(featnames(dfm_text),dict))
to_plot=subset(df, select = c(nomi_colonne))
to_plot=colSums(to_plot)
occ=data.frame(to_plot)
parole_cibo=rownames(occ_cucina)[occ_cucina$to_plot>5]
parole_altro=rownames(occ)[occ$to_plot>150]
filtro=c(parole_altro,parole_cibo)
set_parole=df[filtro]
x_train <- cbind(set_parole, trainino)
x_train=data.table(x_train)
indici=sample(seq_len(nrow(trainino)), size = 7000)
train=x_train[indici]
test=x_train[-indici]
xgb <- RFTrainer$new(n_estimators = 300,classification=1)
xgb$fit(train, "retweet_count")
predictions <- xgb$predict(test)
predictions[1:10]
sum(predictions!=test$retweet_count)
sum(predictions!=test$retweet_count)/10125
sum(predictions!=test$retweet_count)/nrow(test)
user=c("smittenkitchen","GordonRamsay","heyadamroberts","thedomesticman","nomnompaleo",
"Food52","balancedbites","CookingChannel")
dataset=get_timeline("foodwishes",n=35000,include_rts=FALSE,token=twitter_token)
for(i in user) {
temp=get_timeline(i,n=35000,include_rts=FALSE,token=twitter_token)
dataset=rbind.data.frame(dataset,temp)
}
user=c("smittenkitchen","GordonRamsay","heyadamroberts","thedomesticman","nomnompaleo",
"Food52","balancedbites","CookingChannel",
"BoulderLocavore","kyleecooks","cakewhiz","Colmogorman","TaraNoland",
"everyday_eileen","EasyCookin2012","mommye","LivingSMoments")
dataset=get_timeline("foodwishes",n=35000,include_rts=FALSE,token=twitter_token)
for(i in user) {
temp=get_timeline(i,n=35000,include_rts=FALSE,token=twitter_token)
dataset=rbind.data.frame(dataset,temp)
}
text=dataset$text
text=gsub("http[^>]+$"," ",text) #remove last link
text=gsub("http[^>]+ "," ",text) #remove other link
text=gsub("@[^>]+$"," ",text) #remove last @
text=gsub("@[^>]+ "," ",text) #remove other @
###rimuovere punteggiatura per bene, numeri, etc
text=gsub("#"," ",text)
text=tolower(text)
text=str_replace_all(text, "[\r\n]" , " ")
text=stemDocument(text, language = "english")
text=removeWords(text,stopwords(source = "smart")) #removeWords(text,stopwords("en"))
text=stripWhitespace(text)
#text=removeNumbers(text)
#text=gsub("-"," ",text)
#text=gsub("'"," ",text)
text=removePunctuation(text)
dict=readLines("food_corrected.txt",n=725)
dict=unique(dict)
dict=stemDocument(dict, language = "english")
dict=tolower(dict)
text=corpus(text)
token_text <- tokens(text)
dfm_text <- dfm(token_text)
df=convert(dfm_text,to="data.frame")
nomi_colonne=intersect(names(df),dict)
df=subset(df, select = c("doc_id",nomi_colonne))
somma=rowSums(df[,2:ncol(df)])
df$somma=somma
df=subset(df,somma>0)
df$doc_id=gsub("text","",df$doc_id)
df$doc_id=as.numeric(df$doc_id)
dataset=dataset[df$doc_id,]
spettro=read.csv("out.csv")
sum(dataset$reply_count>10
)
sum(dataset$retweet_count>10
)
sum(dataset$retweet_count>5)
sum(dataset$retweet_count>8)
sum(dataset$retweet_count>4)
sum(dataset$favorite_count>10)
aaa=search_tweets(q="food",n = 100,type = "recent",include_rts = FALSE,langs="en",token = twitter_token)
sum(aaa$retweet_count>10)
sum(aaa$retweet_count>1)
View(aaa)
aaa=search_tweets(q="food",n = 100,type = "popular",include_rts = FALSE,langs="en",token = twitter_token)
sum(aaa$retweet_count>1)
sum(aaa$retweet_count>10)
sum(aaa$retweet_count>100)
View(aaa)
aaa=search_tweets(q="food",n = 100000,type = "popular",include_rts = FALSE,langs="en",token = twitter_token)
aaa
View(aaa)
aaa=search_tweets(q="food OR cake OR recipe OR meal OR foodporn",n = 100000,type = "popular",include_rts = FALSE,langs="en",token = twitter_token)
View(aaa)
aaa=search_tweets(q="#food OR #cake OR #recipe OR #meal OR #foodporn",n = 100000,type = "popular",include_rts = FALSE,langs="en",token = twitter_token)
ts_plot(aaa)
aaa=search_tweets(q="#food OR #cake OR #recipe OR #meal OR #foodporn",n = 100000,type = "recent",include_rts = FALSE,langs="en",token = twitter_token)
aaa=search_tweets(q="#food OR #cake OR #recipe OR #meal OR #foodporn",n = 100000,type = "recent",include_rts = FALSE,langs="en",token = twitter_token,retryonratelimit = TRUE)
rm(dataset)
dataset=aaa
View(aaa)
sum(aaa$retweet_count>10)
sum(aaa$retweet_count>5)
sum(aaa$retweet_count>3)
sum(aaa$retweet_count>2)
sum(aaa$retweet_count>1)
sum(aaa$retweet_count>0)
View(aaa)
sum(aaa$retweet_count==0)
sum(aaa$retweet_count==NA)
text=dataset$text
text=gsub("http[^>]+$"," ",text) #remove last link
text=gsub("http[^>]+ "," ",text) #remove other link
text=gsub("@[^>]+$"," ",text) #remove last @
text=gsub("@[^>]+ "," ",text) #remove other @
###rimuovere punteggiatura per bene, numeri, etc
text=gsub("#"," ",text)
text=tolower(text)
text=str_replace_all(text, "[\r\n]" , " ")
text=stemDocument(text, language = "english")
text=removeWords(text,stopwords(source = "smart")) #removeWords(text,stopwords("en"))
text=stripWhitespace(text)
#text=removeNumbers(text)
#text=gsub("-"," ",text)
#text=gsub("'"," ",text)
text=removePunctuation(text)
dict=readLines("food_corrected.txt",n=725)
dict=unique(dict)
dict=stemDocument(dict, language = "english")
dict=tolower(dict)
text=corpus(text)
token_text <- tokens(text)
dfm_text <- dfm(token_text)
text=corpus(text)
token_text <- tokens(text)
dfm_text <- dfm(token_text)
df=convert(dfm_text,to="data.frame")
df=convert(dfm_text,to="data.frame")
rm(train)
View(aaa)
df=convert(dfm_text,to="data.frame")
nomi_colonne=intersect(names(df),dict)
df=subset(df, select = c("doc_id",nomi_colonne))
somma=rowSums(df[,2:ncol(df)])
df$somma=somma
